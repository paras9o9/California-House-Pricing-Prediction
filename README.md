# ğŸ¡ California Housing Price Prediction using Linear, Lasso, and Random Forest Regression

This project predicts median house values in California using multiple regression techniques, emphasizing the complete ML workflow â€” from raw data to model deployment. It includes comprehensive data preprocessing, exploratory analysis, feature engineering, model training, evaluation, and visualization.

---

## ğŸ“Œ Overview

I used the **California Housing Prices dataset** to build and evaluate three machine learning models:

- **Linear Regression**
- **Lasso Regression** (with alpha tuning via cross-validation)
- **Random Forest Regressor** (with hyperparameter tuning via `RandomizedSearchCV`)

### ğŸ” Key Insights:

- **Linear Regression** captured general trends  
  ğŸ‘‰ Test RÂ²: **0.57**
- **Lasso Regression** improved generalization  
  ğŸ‘‰ Test RÂ²: **0.60**
- **Random Forest Regressor** gave the best performance  
  ğŸ‘‰ Test RÂ²: **0.83**

### ğŸ“ˆ Most Influential Features:

- `median_income`
- `ocean_proximity_INLAND`
- `population_per_households`
- `latitude` & `longitude`

Residual analysis confirmed good model fit with minimal bias and acceptable variance.

---

## âš™ï¸ Tech Stack

- **Programming Language**: Python ğŸ

### Libraries:
- `pandas`, `NumPy` â€“ Data loading and processing  
- `matplotlib`, `seaborn` â€“ Visualization  
- `scikit-learn` â€“ ML models, evaluation, and preprocessing  
- `joblib` â€“ Model persistence  
- **Environment**: Google Colab / Jupyter Notebook

---

## ğŸ“‚ Dataset

- **Source**: [Kaggle - California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)
- **Format**: CSV file

---

## ğŸ—‚ï¸ Project Structure

```

California-House-Pricing-Prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ housing.csv                      # Raw dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ HousePricingPrediction.ipynb     # Main analysis and modeling notebook
â”‚
â”œâ”€â”€ plots/                               # Visualizations generated in the notebook
â”‚   â”œâ”€â”€ linear\_coefficients.png
â”‚   â”œâ”€â”€ lasso\_coefficients.png
â”‚   â”œâ”€â”€ rf\_feature\_importance.png
â”‚   â””â”€â”€ residuals\_plot.png
â”‚
â”œâ”€â”€ random\_forest\_model.pkl              # Saved trained model
â”œâ”€â”€ requirements.txt                     # Python dependencies
â””â”€â”€ README.md                            # Project documentation

---

## ğŸ“Š Evaluation Metrics

| Model               | Test RÂ² | Test MSE |
|--------------------|---------|----------|
| Linear Regression   | 0.5747  | 0.1391   |
| Lasso Regression    | 0.5991  | 0.1311   |
| Random Forest       | 0.8377  | 0.0531   |
| Tuned RF (Best)     | **0.8301** | **0.0556** |

---

## ğŸ’¡ Key Features

- **Log transformation** and **feature scaling** to improve model performance
- **One-Hot Encoding** for categorical variable (`ocean_proximity`)
- Custom feature engineering:  
  `room_per_households`, `bedroom_per_room`, `population_per_households`
- Residual plots and feature importance visualizations

---

## ğŸ§  Lessons Learned

- Lasso helped reduce overfitting by penalizing less important features.
- Ensemble methods like Random Forest captured non-linear relationships effectively.
- Preprocessing (especially handling skew and scale) significantly improved performance.
